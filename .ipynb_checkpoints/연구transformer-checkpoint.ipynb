{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139f44e-9435-4a36-b290-464fa9580e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e032c4f-de64-4779-b7f2-d790c7301526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import lightning as L\n",
    "\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from torchvision.transforms import v2 as  transforms\n",
    "from torchvision.io import read_image\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef7b89-0c36-4660-a802-0293caa397e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7aa2b-691a-418c-8529-deaa61b710e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_matrix(k, alpha=0.3):\n",
    "    \"\"\"r\n",
    "    Quality matrix Q. Described in the eq (17) so that eps = QX, where X is the input. \n",
    "    Alpha is 0.3, as mentioned in Appendix D.\n",
    "    \"\"\"\n",
    "    identity = torch.diag(torch.ones(k))\n",
    "    shift_identity = torch.zeros(k, k)\n",
    "    for i in range(k):\n",
    "        shift_identity[(i+1)%k, i] = 1\n",
    "    opt = -alpha * identity + alpha * shift_identity\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75357a-98d8-47e2-a7ab-9c7747a287de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import Block, Attention, VisionTransformer\n",
    "\n",
    "def attention_forward(self, x, attn_bias=None):\n",
    "    B, N, C = x.shape\n",
    "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv.unbind(0)\n",
    "    q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "    q = q * self.scale\n",
    "    attn = q @ k.transpose(-2, -1)\n",
    "    if attn_bias is not None:\n",
    "        attn = attn + attn_bias\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1, 2).reshape(B, N, C)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "    return x\n",
    "Attention.forward = attention_forward\n",
    "\n",
    "def block_forward(self, x_and_attn_bias):\n",
    "    x, attn_bias = x_and_attn_bias\n",
    "    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_bias)))\n",
    "    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "    return (x, attn_bias)\n",
    "Block.forward = block_forward\n",
    "\n",
    "def vision_transformer_forward_features(self, x, embed_bias=None, attn_bias=None):\n",
    "    x = self.patch_embed(x)\n",
    "    x = self._pos_embed(x)\n",
    "    if embed_bias is not None:\n",
    "        x = x + embed_bias\n",
    "    x = self.patch_drop(x)\n",
    "    x = self.norm_pre(x)\n",
    "  # noisyNN\n",
    "    x, attn_bias = self.blocks[:-1]((x,attn_bias))\n",
    "    x = x.permute(0, 2, 1)\n",
    "    B, C, L = x.shape\n",
    "    x = x.reshape(B, C, self.stage3_res, self.stage3_res).contiguous()\n",
    "    x = self.linear_transform_noise@x + x\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    x, _ = self.blocks[-1]((x,attn_bias))\n",
    "    # x, _ = self.blocks((x,attn_bias))\n",
    "    x = self.norm(x)\n",
    "    return x\n",
    "VisionTransformer.forward_features = vision_transformer_forward_features\n",
    "\n",
    "def vision_transformer_forward(self, x, embed_bias=None, attn_bias=None):\n",
    "    x = self.forward_features(x, embed_bias, attn_bias)\n",
    "    return x\n",
    "VisionTransformer.forward = vision_transformer_forward\n",
    "\n",
    "model = timm.create_model('vit_medium_patch16_gap_256', pretrained=True, num_classes=0)\n",
    "\n",
    "# noisyNN\n",
    "model.stage3_res = 256//16\n",
    "model.linear_transform_noise = torch.nn.Parameter(quality_matrix(model.stage3_res), requires_grad=False)\n",
    "\n",
    "model_config = {\n",
    "    'image_size':256,\n",
    "    'patch_size':16,\n",
    "    'hidden_size':512,\n",
    "    'num_attention_heads':8,\n",
    "}\n",
    "\n",
    "pretrained_transform_config = timm.data.resolve_data_config(model.pretrained_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8deb1-da03-46d9-a45f-192cb92ef5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['seed']=42\n",
    "config['batch_size']=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103566a7-a512-41dd-ab0a-e8d760c90cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b615eb-0aeb-4f57-a823-0f219fa4b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2fac8-8f0c-4f80-ab86-f741842a73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43243b28-8aad-4905-8e64-cd0abdf47fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_idx = 0\n",
    "for idx, (train_indices, val_indices) in enumerate(kf.split(train_df)):\n",
    "    if idx==fold_idx:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeddca3-98fe-4575-ad2d-ae84ef3dc0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_df.iloc[train_indices], train_df.iloc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4de7a5-23b5-4eb7-a12a-973bfaa56114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, df, data_path, mode='train'):\n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path']))\n",
    "            shuffle_order = row[[str(i) for i in range(1, 17)]].values-1\n",
    "            image_src = self.reset_image(image, shuffle_order)\n",
    "            image_transform = self.train_transform(image_src)\n",
    "            image_reshuffle, reshuffle_order = self.shuffle_image(image_transform)\n",
    "            adjacency_matrix = self.get_adjacency_matrix(reshuffle_order)\n",
    "            data = {\n",
    "                'image_reshuffle':image_reshuffle,\n",
    "                'order':reshuffle_order,\n",
    "                'adjacency_matrix':adjacency_matrix,\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'val':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path'])).numpy()\n",
    "            shuffle_order = row[[str(i) for i in range(1, 17)]].values-1\n",
    "            adjacency_matrix = self.get_adjacency_matrix(shuffle_order.tolist())\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'order':shuffle_order,\n",
    "                'adjacency_matrix':adjacency_matrix,\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'inference':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path'])).numpy()\n",
    "            data = {\n",
    "                'image':image\n",
    "            }\n",
    "            return data\n",
    "\n",
    "    def train_transform(self, image):\n",
    "        if random.random()>0.5: #hflip\n",
    "            image = image[:,:, ::-1]\n",
    "        if random.random()>0.5: #vflip\n",
    "            image = image[:,::-1, :]\n",
    "        if random.random()>0.5: #transpose\n",
    "            image = image.transpose(0,2,1)\n",
    "        return image\n",
    "        \n",
    "    def reset_image(self, image, shuffle_order):\n",
    "        c, h, w = image.shape\n",
    "        block_h, block_w = h//4, w//4\n",
    "        image_src = [[0 for _ in range(4)] for _ in range(4)]\n",
    "        for idx, order in enumerate(shuffle_order):\n",
    "            h_idx, w_idx = divmod(order,4)\n",
    "            h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "            image_src[h_idx][w_idx] = image[:, block_h * h_idx_shuffle : block_h * (h_idx_shuffle+1), block_w * w_idx_shuffle : block_w * (w_idx_shuffle+1)]\n",
    "        image_src = np.concatenate([np.concatenate(image_row, -1) for image_row in image_src], -2)\n",
    "        return image_src\n",
    "\n",
    "    def shuffle_image(self, image):\n",
    "        c, h, w = image.shape\n",
    "        block_h, block_w = h//4, w//4\n",
    "        shuffle_order = list(range(0, 16))\n",
    "        \n",
    "        elements = list(range(1,17))\n",
    "        probabilities = [i**0.5 for i in range(1,17)]\n",
    "        n = random.choices(elements, weights=probabilities, k=1)[0]\n",
    "        selected_indices = random.sample(range(len(shuffle_order)), n)\n",
    "        remaining_indices = [i for i in range(len(shuffle_order)) if i not in selected_indices]\n",
    "        shuffled_selected_elements = [shuffle_order[i] for i in selected_indices]\n",
    "        random.shuffle(shuffled_selected_elements)\n",
    "        shuffle_order = [shuffled_selected_elements[selected_indices.index(i)] if i in selected_indices else shuffle_order[i] for i in range(len(shuffle_order))]\n",
    "    \n",
    "        image_shuffle = [[0 for _ in range(4)] for _ in range(4)]\n",
    "        for idx, order in enumerate(shuffle_order):\n",
    "            h_idx, w_idx = divmod(order,4)\n",
    "            h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "            image_shuffle[h_idx_shuffle][w_idx_shuffle] = image[:, block_h * h_idx : block_h * (h_idx+1), block_w * w_idx : block_w * (w_idx+1)]\n",
    "        image_shuffle = np.concatenate([np.concatenate(image_row, -1) for image_row in image_shuffle], -2)\n",
    "        return image_shuffle, shuffle_order\n",
    "    \n",
    "    def get_adjacency_matrix(self, order): # 패치에 대하여 연결된 패치 찾기\n",
    "        order_matrix = [order[4*i:4*(i+1)]for i in range(4)]\n",
    "        adj_matrix = np.zeros((16,16), dtype=int)\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                o = order_matrix[i][j]\n",
    "                i_o, j_o = divmod(o,4)\n",
    "                for i_add,j_add in [(-1,0), (1,0), (0,1), (0,-1)]:\n",
    "                    i_compare, j_compare = i_o+i_add, j_o+j_add\n",
    "                    if i_compare<0 or i_compare>=4 or j_compare<0 or j_compare>=4 : continue\n",
    "                    o_compare = order[i_compare*4+j_compare]\n",
    "                    i_, j_ = i*4+j, order.index(i_compare*4+j_compare)\n",
    "                    if (i_add,j_add) == (-1,0):\n",
    "                        adj_matrix[i_][j_] = 1 # 상\n",
    "                        adj_matrix[j_][i_] = 2 # 하\n",
    "                    elif (i_add,j_add) == (-1,0):\n",
    "                        adj_matrix[i_][j_] = 2\n",
    "                        adj_matrix[j_][i_] = 1\n",
    "                    elif  (i_add,j_add) == (0,-1):\n",
    "                        adj_matrix[i_][j_] = 3 # 좌\n",
    "                        adj_matrix[j_][i_] = 4 # 우\n",
    "                    elif (i_add,j_add) == (0,1):\n",
    "                        adj_matrix[i_][j_] = 4\n",
    "                        adj_matrix[j_][i_] = 3\n",
    "        return adj_matrix\n",
    "\n",
    "    def connect_to_piece(self, connect_types):\n",
    "        piece_type = []\n",
    "        for connect_type_row in connect_types:\n",
    "            connect_bins = tuple(np.bincount(connect_type_row, minlength=5)[1:])\n",
    "            if connect_bins == (0,1,0,1): #  ┌\n",
    "                piece_type.append(1)\n",
    "            elif connect_bins == (0,1,1,1): # ㅜ\n",
    "                piece_type.append(2)\n",
    "            elif connect_bins == (0,1,1,0): # ㄱ\n",
    "                piece_type.append(3)\n",
    "            elif connect_bins == (1,1,0,1): # ㅏ\n",
    "                piece_type.append(4)\n",
    "            elif connect_bins == (1,1,1,0): # ㅓ\n",
    "                piece_type.append(5)\n",
    "            elif connect_bins == (1,0,0,1): # ㄴ\n",
    "                piece_type.append(6)\n",
    "            elif connect_bins == (1,0,1,1): # ㅗ\n",
    "                piece_type.append(7)\n",
    "            elif connect_bins == (1,0,1,0): # ┘\n",
    "                piece_type.append(8)\n",
    "            elif connect_bins == (1,1,1,1): # +\n",
    "                piece_type.append(9)\n",
    "            else: # unknown\n",
    "                piece_type.append(0)\n",
    "        piece_type = np.array(piece_type)\n",
    "        return piece_type\n",
    "    \n",
    "    def get_score(self, order_true, order_pred): # regression task? 현재 아키텍처와 맞지 않을듯\n",
    "        puzzle_a = np.array(order_true, dtype=int).reshape(4, 4)\n",
    "        puzzle_s = np.array(order_pred, dtype=int).reshape(4, 4)\n",
    "\n",
    "        accuracies = {}\n",
    "        accuracies['1x1'] = np.mean(puzzle_a == puzzle_s)\n",
    "\n",
    "        combinations_2x2 = [(i, j) for i in range(3) for j in range(3)]\n",
    "        combinations_3x3 = [(i, j) for i in range(2) for j in range(2)]\n",
    "\n",
    "        for size in range(2, 5):  # Loop through sizes 2, 3, 4\n",
    "            correct_count = 0  # Initialize counter for correct full sub-puzzles\n",
    "            total_subpuzzles = 0\n",
    "            combinations = combinations_2x2 if size == 2 else combinations_3x3 if size == 3 else [(0, 0)]\n",
    "            for start_row, start_col in combinations:\n",
    "                rows = slice(start_row, start_row + size)\n",
    "                cols = slice(start_col, start_col + size)\n",
    "                if np.array_equal(puzzle_a[rows, cols], puzzle_s[rows, cols]):\n",
    "                    correct_count += 1\n",
    "                total_subpuzzles += 1\n",
    "\n",
    "            accuracies[f'{size}x{size}'] = correct_count / total_subpuzzles\n",
    "\n",
    "        score = (accuracies['1x1'] + accuracies['2x2'] + accuracies['3x3'] + accuracies['4x4']) / 4.\n",
    "        return score, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550235a-5960-474b-97e3-45ce129a3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawCollateFn:\n",
    "    def __init__(self, transform, mode):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.mode=='train':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image_reshuffle'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            order = torch.LongTensor([data['order'] for data in batch])\n",
    "            adjacency_matrx = torch.LongTensor([data['adjacency_matrix'] for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'order':order,\n",
    "                'adjacency_matrx':adjacency_matrx,\n",
    "            }\n",
    "        elif self.mode=='val':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            order = torch.LongTensor([data['order'] for data in batch])\n",
    "            adjacency_matrx = torch.LongTensor([data['adjacency_matrix'] for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'order':order,\n",
    "                'adjacency_matrx':adjacency_matrx,\n",
    "            }\n",
    "        elif self.mode=='inference':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691780ae-1366-4965-8a56-83f9290df0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawAug(nn.Module):\n",
    "    def __init__(self,image_size, block_size, how='inner', ratio=0.1):\n",
    "        super(JigsawAug, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.block_size = block_size\n",
    "        self.n_blocks = image_size // block_size\n",
    "        self.how = how\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, image):\n",
    "        image = image.numpy()\n",
    "        c, h, w = image.shape\n",
    "        block_images = image.reshape(c, self.n_blocks, self.block_size, self.n_blocks, self.block_size)\n",
    "        block_images = block_images.transpose(1, 3, 0, 2, 4).reshape(self.n_blocks**2, c, self.block_size, self.block_size)\n",
    "        block_images = np.array([self.apply_block_noise(block_image, self.how) for block_image in block_images])\n",
    "        transformed_image = block_images.reshape(self.n_blocks, self.n_blocks, c, self.block_size, self.block_size).transpose(2, 0, 3, 1, 4).reshape(c, h, w)\n",
    "        transformed_image = torch.from_numpy(transformed_image).float()\n",
    "        return transformed_image\n",
    "\n",
    "    def apply_block_noise(self, block_image, how):\n",
    "        if random.random()>self.ratio : \n",
    "            return block_image\n",
    "        elif how == 'outer':\n",
    "            block_size_half = (self.block_size-1)/2\n",
    "            a = block_size_half - abs(np.arange(0, self.block_size) - block_size_half)\n",
    "            b = a.reshape(-1,1)\n",
    "            a = a.reshape(1,-1)\n",
    "            dist_matrix = (self.block_size-1) -  np.where(b > a, a, b)\n",
    "            prob_matrix = dist_matrix/(self.block_size-1)\n",
    "            prob_matrix = prob_matrix**2\n",
    "            \n",
    "            bool_matrix = np.random.rand(*prob_matrix.shape) > prob_matrix\n",
    "            block_image = block_image * (bool_matrix*1)\n",
    "            return block_image\n",
    "        elif how == 'inner':\n",
    "            block_size_half = (self.block_size-1)/2\n",
    "            a = block_size_half - abs(np.arange(0, self.block_size) - block_size_half)\n",
    "            b = a.reshape(-1,1)\n",
    "            a = a.reshape(1,-1)\n",
    "            dist_matrix = np.sqrt(a**2 + b**2)\n",
    "            dist_max = dist_matrix.max()\n",
    "            prob_matrix = dist_matrix/dist_max\n",
    "            prob_matrix = prob_matrix**2\n",
    "            bool_matrix = np.random.rand(*prob_matrix.shape) > prob_matrix\n",
    "            block_image = block_image * (bool_matrix*1)\n",
    "            return block_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0dda9-0006-44e7-950a-97b81439fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JigsawDataset(\n",
    "    df=train_df,\n",
    "    data_path='./data',\n",
    "    mode='train'\n",
    ")\n",
    "val_dataset = JigsawDataset(\n",
    "    df=val_df,\n",
    "    data_path='./data',\n",
    "    mode='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abcf7d-6045-4755-93a0-ad2229235a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_transform_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4802e3-6d19-4371-bed6-1da226a58de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    JigsawAug(image_size=256, block_size=256//4, how='outer', ratio=0.1),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554a4eb-c49a-4bbc-a629-56797106836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=JigsawCollateFn(train_transform, 'train'), batch_size=config['batch_size'])\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=JigsawCollateFn(val_transform, 'val'), batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b3e0a-87be-417b-93e9-46e8004ffc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    1st Stage:\n",
    "    In the initial stage, a transformer architecture is employed to discern optimal patch arrangements for each puzzle segment.\n",
    "    This involves intricate spatial relationships, where the model dynamically identifies neighboring patches in cardinal directions(i.e., up, down, left, right).\n",
    "    The foundation of this stage lies in the incorporation of attention matrices at the final layer, providing nuanced insights into patch interdependencies.\n",
    "    \n",
    "    2nd Stage:\n",
    "    Subsequently, the second stage capitalizes on the predicted matrices from the initial stage to derive piece-type embeddings and connect-type embedding.\n",
    "    These embeddings encapsulate diverse spatial configurations, such as cross shapes, left corners and right, and else.\n",
    "    The innovation lies in the integration of piece-type embeddings as positional embedding biases, enhancing the model's contextual awareness.\n",
    "    Furthermore, connect matrix embeddings serve as attention biases, enabling the model to capture intricate inter-piece relationships.\n",
    "    The final objective of this stage is to predict an optimal reordering sequence, leveraging the acquired embeddings.\n",
    "    \n",
    "    The backbone model shares weights excluding head layers. And losses are jointly computed for gradient updates, aiming for efficient learning and high performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        super(JigsawFormer, self).__init__()\n",
    "        for k,v in config.items():\n",
    "            setattr(self,k,v)\n",
    "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
    "        self.num_patch_per_block = int(self.image_size/4/self.patch_size)\n",
    "        self.model = model\n",
    "        \n",
    "        self.pos_emb = nn.Parameter(torch.randn(16, self.hidden_size))\n",
    "        self.piece_type_emb = nn.Embedding(10, self.hidden_size, padding_idx=0)\n",
    "        self.piece_type_emb.weight.data[0,:]=0\n",
    "        self.piece_type_emb.weight.data = self.piece_type_emb.weight.data*0.1\n",
    "        self.connect_type_emb = nn.Embedding(5, self.num_attention_heads, padding_idx=0)\n",
    "        self.connect_type_emb.weight.data[0,:]=0\n",
    "        self.connect_type_emb.weight.data = self.connect_type_emb.weight.data*0.1\n",
    "        \n",
    "        self.local_linear1 = nn.LazyLinear(self.hidden_size)\n",
    "        self.local_linear2 = nn.LazyLinear(self.hidden_size)\n",
    "        self.local_conv = nn.Conv2d(self.num_attention_heads, self.num_attention_heads, int((self.image_size**2)/(16**3)), int((self.image_size**2)/(16**3)))\n",
    "        self.local_clf = nn.Sequential(\n",
    "            nn.LazyLinear(self.num_attention_heads),\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(5),\n",
    "        )\n",
    "\n",
    "        self.global_conv = nn.Conv1d(self.hidden_size, self.hidden_size, int((self.image_size**2)/(16**3)), int((self.image_size**2)/(16**3)))\n",
    "        self.global_clf = nn.Sequential(\n",
    "            nn.LazyLinear(self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(16),\n",
    "        )\n",
    "\n",
    "\n",
    "    def _transpose(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        b, h, l, d = x.shape\n",
    "        x = torch.cat(x.reshape(b, h, -1, self.num_patch_per_block, d).split(self.num_patch_per_block, 2), 3).reshape(b, h, l, d)\n",
    "        return x\n",
    "\n",
    "#     @torch.compile\n",
    "    def local_forward(self, x, label=None):\n",
    "        pos_emb = self.pos_emb.reshape(4,4,-1)\n",
    "        pos_emb = pos_emb.unsqueeze(-2).repeat(1,1,self.num_patch_per_block,1).reshape(4,-1,self.hidden_size)\n",
    "        pos_emb = pos_emb.unsqueeze(1).repeat(1,self.num_patch_per_block, 1, 1).reshape(-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "        pos_emb = pos_emb.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        x = self.model(x, embed_bias=pos_emb)\n",
    "        x1 = self._transpose(self.local_linear1(x))\n",
    "        x2 = self._transpose(self.local_linear2(x))\n",
    "        x = torch.matmul(x1,x2.transpose(-1, -2)).transpose(-1,-2)\n",
    "        x = self.local_conv(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = self.local_clf(x)\n",
    "        probs = nn.Softmax(dim=-1)(x)\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x.reshape(-1, 5), label.reshape(-1))\n",
    "        return x, probs, loss\n",
    "\n",
    "#     @torch.compile\n",
    "    def global_forward(self, x, piece_type=None, connect_type=None, label=None):\n",
    "        pos_emb = self.pos_emb.reshape(4,4,-1)\n",
    "        pos_emb = pos_emb.unsqueeze(-2).repeat(1,1,self.num_patch_per_block,1).reshape(4,-1,self.hidden_size)\n",
    "        pos_emb = pos_emb.unsqueeze(1).repeat(1,self.num_patch_per_block, 1, 1).reshape(-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "        pos_emb = pos_emb.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        if piece_type is not None:\n",
    "            b = piece_type.shape[0]\n",
    "            piece_emb = self.piece_type_emb(piece_type).reshape(b, 4, 4, -1)\n",
    "            piece_emb = piece_emb.unsqueeze(-2).repeat(1,1,1,self.num_patch_per_block,1).reshape(b, 4,-1,self.hidden_size)\n",
    "            piece_emb = piece_emb.unsqueeze(2).repeat(1,1,self.num_patch_per_block, 1, 1).reshape(b,-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "            piece_emb = piece_emb.reshape(b,-1, self.hidden_size)\n",
    "            pos_emb = piece_emb+pos_emb\n",
    "            \n",
    "        attn_bias = None\n",
    "        if connect_type is not None:\n",
    "            b = connect_type.shape[0] # b 16 16 5\n",
    "            attn_bias = self.connect_type_emb(connect_type) # B 16,16,5\n",
    "            attn_bias = attn_bias.unsqueeze(-2).repeat(1,1,1,int(self.num_patch_per_block**2),1).reshape(b,16,-1,self.num_attention_heads)\n",
    "            attn_bias = attn_bias.unsqueeze(2).repeat(1,1,int(self.num_patch_per_block**2), 1, 1).reshape(b,-1, int((self.image_size/16)**2), self.num_attention_heads)\n",
    "            attn_bias = attn_bias.permute(0,3,1,2)\n",
    "            \n",
    "        x = self.model(\n",
    "            x,\n",
    "            embed_bias=pos_emb,\n",
    "            attn_bias=attn_bias,\n",
    "        )\n",
    "        x = self._transpose(x)\n",
    "        b, h, l, d = x.shape\n",
    "        x = x.permute(0,1,3,2).reshape(b,h*d,l)\n",
    "        x = self.global_conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x_out = self.global_clf(x)\n",
    "        probs = nn.Softmax(dim=-1)(x_out)\n",
    "        \n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x_out.reshape(-1, 16), label.reshape(-1))\n",
    "        return x_out, probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447e65b-dab7-49d2-aa18-8c4cc96142df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitJigsawFormer(L.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = JigsawFormer(model, config)\n",
    "        self.train_iter = 2\n",
    "        self.val_iter = 5\n",
    "        self.validation_step_outputs = [[] for _ in range(self.val_iter)]\n",
    "        self.inference_iter = 2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return opt\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        adjacency_matrx = batch['adjacency_matrx']\n",
    "        order = batch['order']\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(self.train_iter):\n",
    "            connect_logits, connect_probs, loss_connect = self.model.local_forward(pixel_values, adjacency_matrx)\n",
    "            connect_type = connect_probs.argmax(-1).detach()\n",
    "            piece_type = self._connect_to_piece(connect_type)\n",
    "        \n",
    "            order_logits, order_probs, loss_order = self.model.global_forward(\n",
    "                pixel_values, \n",
    "                piece_type=piece_type,\n",
    "                connect_type=connect_type, \n",
    "                label=order, \n",
    "            )\n",
    "            order_pred = self._probs_to_order(order_probs)\n",
    "            pixel_values = self._reorder_image(pixel_values, order_pred).detach()\n",
    "            order = self._get_order_true_new(order_pred, order).detach()\n",
    "            adjacency_matrx = self._get_adjacency_matrix(order).detach()\n",
    "            loss += loss_connect + loss_order\n",
    "            \n",
    "            self.log(f\"train_loss_connect_stage={i+1}\", loss_connect, on_step=True, on_epoch=False)\n",
    "            self.log(f\"train_loss_order_stage={i+1}\", loss_order, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        adjacency_matrx = batch['adjacency_matrx']\n",
    "        order = batch['order']\n",
    "        \n",
    "        for i in range(self.val_iter):\n",
    "            connect_logits, connect_probs, loss_connect = self.model.local_forward(pixel_values, adjacency_matrx)\n",
    "            self.log(f\"val_loss_connect_stage={i+1}\", loss_connect)\n",
    "            connect_type = connect_probs.argmax(-1).detach()\n",
    "            connect_accuracy = torch.mean(1*(connect_type == adjacency_matrx), dtype=torch.float32)\n",
    "            self.log(f\"val_connect_accuracy_stage={i+1}\", connect_accuracy)\n",
    "            piece_type = self._connect_to_piece(connect_type)\n",
    "            \n",
    "            order_logits, order_probs, loss_order = self.model.global_forward(\n",
    "                pixel_values, \n",
    "                piece_type=piece_type,\n",
    "                connect_type=connect_type, \n",
    "                label=order, \n",
    "            )\n",
    "            self.validation_step_outputs[i].append((order_probs, order))\n",
    "            self.log(f\"val_loss_order_stage={i+1}\", loss_order)\n",
    "            \n",
    "            order_pred = self._probs_to_order(order_probs)\n",
    "            pixel_values = self._reorder_image(pixel_values, order_pred).detach()\n",
    "            order = self._get_order_true_new(order_pred, order).detach()\n",
    "            adjacency_matrx = self._get_adjacency_matrix(order).detach()\n",
    "        return \n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        order = [None] * pixel_values.shape[0]\n",
    "        \n",
    "        for i in range(self.inference_iter):\n",
    "            connect_logits, connect_probs, _ = self.model.local_forward(pixel_values)\n",
    "            connect_type = connect_probs.argmax(-1).detach()\n",
    "            piece_type = self._connect_to_piece(connect_type)\n",
    "            \n",
    "            order_logits, order_probs, _ = self.model.global_forward(\n",
    "                pixel_values, \n",
    "                piece_type=piece_type,\n",
    "                connect_type=connect_type, \n",
    "            )            \n",
    "            order_pred = self._probs_to_order(order_probs)\n",
    "            order = np.array([self._merge_preds(o1, o2) for o1, o2 in zip(order, order_pred.detach().cpu().numpy())])\n",
    "            pixel_values = self._reorder_image(pixel_values, order_pred).detach()\n",
    "            \n",
    "        return order\n",
    "    \n",
    "    def _merge_preds(self, pred1, pred2):\n",
    "        if pred1 is None:\n",
    "            return pred2\n",
    "        else:\n",
    "            return [int(pred2[p1]) for p1 in pred1]\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        for idx, validation_step_output in enumerate(self.validation_step_outputs):\n",
    "            order_pred = []\n",
    "            order_true = []\n",
    "            for probs, order in validation_step_output:\n",
    "                order_pred.append(self._probs_to_order(probs))\n",
    "                order_true.append(order)\n",
    "            order_pred = torch.cat(order_pred).detach().cpu().numpy()\n",
    "            order_true = torch.cat(order_true).detach().cpu().numpy()\n",
    "\n",
    "            combinations_2x2 = [(i, j) for i in range(3) for j in range(3)]\n",
    "            combinations_3x3 = [(i, j) for i in range(2) for j in range(2)]\n",
    "            accuracies = {}\n",
    "            accuracies['1x1'] = np.mean(order_true == order_pred)\n",
    "\n",
    "            for size in range(2, 5): \n",
    "                correct_count = 0  \n",
    "                total_subpuzzles = 0\n",
    "                for i in range(len(order_true)):\n",
    "                    puzzle_a = order_true[i].reshape(4, 4)\n",
    "                    puzzle_s = order_pred[i].reshape(4, 4)\n",
    "                    combinations = combinations_2x2 if size == 2 else combinations_3x3 if size == 3 else [(0, 0)]\n",
    "                    for start_row, start_col in combinations:\n",
    "                        rows = slice(start_row, start_row + size)\n",
    "                        cols = slice(start_col, start_col + size)\n",
    "                        if np.array_equal(puzzle_a[rows, cols], puzzle_s[rows, cols]):\n",
    "                            correct_count += 1\n",
    "                        total_subpuzzles += 1\n",
    "                accuracies[f'{size}x{size}'] = correct_count / total_subpuzzles\n",
    "            score = (accuracies['1x1'] + accuracies['2x2'] + accuracies['3x3'] + accuracies['4x4']) / 4.\n",
    "\n",
    "            self.log(f\"val_score_1x1_stage={idx+1}\", accuracies['1x1'])\n",
    "            self.log(f\"val_score_stage={idx+1}\", score)\n",
    "            validation_step_output.clear()\n",
    "        self.log(f\"val_score_end\", score)\n",
    "        return\n",
    "    \n",
    "    def _probs_to_order(self, probs): # Greedily arrange the jigsaw puzzle pieces based on maximum probability.\n",
    "        device = probs.device\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        \n",
    "        order = []\n",
    "        for prob in probs:\n",
    "            prob = prob.reshape(16,16)\n",
    "            indices = [-1 for _ in range(16)]\n",
    "            for _ in range(16):\n",
    "                i, j = divmod(int(prob.argmax()),16)\n",
    "                indices[i]=j\n",
    "                prob[i, :] = float('-inf')\n",
    "                prob[:, j] = float('-inf')\n",
    "            order.append(indices)\n",
    "        order = torch.LongTensor(order).to(device)\n",
    "        return order\n",
    "    \n",
    "    def _reorder_image(self, images, reorders):\n",
    "        device = images.device\n",
    "        images = images.detach().cpu().numpy()\n",
    "        reorders = reorders.detach().cpu().numpy()   \n",
    "        \n",
    "        images_reordered = []\n",
    "        for image, reorder in zip(images, reorders):\n",
    "            c, h, w = image.shape\n",
    "            block_h, block_w = h//4, w//4\n",
    "            image_src = [[0 for _ in range(4)] for _ in range(4)]\n",
    "            for idx, order in enumerate(reorder):\n",
    "                h_idx, w_idx = divmod(order,4)\n",
    "                h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "                image_src[h_idx][w_idx] = image[:, block_h * h_idx_shuffle : block_h * (h_idx_shuffle+1), block_w * w_idx_shuffle : block_w * (w_idx_shuffle+1)]\n",
    "            image_reordered = np.concatenate([np.concatenate(image_row, -1) for image_row in image_src], -2)\n",
    "            image_reordered = torch.from_numpy(image_reordered)\n",
    "            images_reordered.append(image_reordered)\n",
    "        images_reordered = torch.stack(images_reordered).to(device)\n",
    "        return images_reordered\n",
    "    \n",
    "    def _get_adjacency_matrix(self, orders): # 패치에 대하여 연결된 패치 찾기\n",
    "        device = orders.device\n",
    "        orders = orders.detach().cpu().numpy().reshape(-1,16).tolist()\n",
    "        adj_matries = []\n",
    "        for order in orders:\n",
    "            order_matrix = [order[4*i:4*(i+1)] for i in range(4)]\n",
    "            adj_matrix = np.zeros((16,16), dtype=int)\n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    o = order_matrix[i][j]\n",
    "                    i_o, j_o = divmod(o,4)\n",
    "                    for i_add,j_add in [(-1,0), (1,0), (0,1), (0,-1)]:\n",
    "                        i_compare, j_compare = i_o+i_add, j_o+j_add\n",
    "                        if i_compare<0 or i_compare>=4 or j_compare<0 or j_compare>=4 : continue\n",
    "                        o_compare = order[i_compare*4+j_compare]\n",
    "                        i_, j_ = i*4+j, order.index(i_compare*4+j_compare)\n",
    "                        if (i_add,j_add) == (-1,0):\n",
    "                            adj_matrix[i_][j_] = 1 # 상\n",
    "                            adj_matrix[j_][i_] = 2 # 하\n",
    "                        elif (i_add,j_add) == (-1,0):\n",
    "                            adj_matrix[i_][j_] = 2\n",
    "                            adj_matrix[j_][i_] = 1\n",
    "                        elif  (i_add,j_add) == (0,-1):\n",
    "                            adj_matrix[i_][j_] = 3 # 좌\n",
    "                            adj_matrix[j_][i_] = 4 # 우\n",
    "                        elif (i_add,j_add) == (0,1):\n",
    "                            adj_matrix[i_][j_] = 4\n",
    "                            adj_matrix[j_][i_] = 3\n",
    "            adj_matries.append(adj_matrix)\n",
    "        adj_matries = torch.LongTensor(adj_matries).to(device)\n",
    "        return adj_matries\n",
    "    \n",
    "    def _get_order_true_new(self, order_preds, order_trues):\n",
    "        device = order_preds.device\n",
    "        order_preds = order_preds.detach().cpu().numpy()\n",
    "        order_trues = order_trues.detach().cpu().numpy()\n",
    "        \n",
    "        orders_true_new = []\n",
    "        for order_true, order_pred in zip(order_preds, order_trues):\n",
    "            order_true_new = [-1 for _ in range(16)]\n",
    "            for idx, o in enumerate(order_pred):\n",
    "                order_true_new[o] = order_true[idx]\n",
    "            orders_true_new.append(order_true_new)\n",
    "        orders_true_new = torch.LongTensor(orders_true_new).to(device)\n",
    "        return orders_true_new\n",
    "    \n",
    "    def _connect_to_piece(self, connect_types):\n",
    "        device = connect_types.device\n",
    "        connect_types = connect_types.detach().cpu().numpy()\n",
    "        piece_types = []\n",
    "        for connect_type in connect_types:\n",
    "            piece_type = []\n",
    "            for connect_type_row in connect_type:\n",
    "                connect_bins = tuple(np.bincount(connect_type_row, minlength=5)[1:])\n",
    "                if connect_bins == (0,1,0,1): #  ┌\n",
    "                    piece_type.append(1)\n",
    "                elif connect_bins == (0,1,1,1): # ㅜ\n",
    "                    piece_type.append(2)\n",
    "                elif connect_bins == (0,1,1,0): # ㄱ\n",
    "                    piece_type.append(3)\n",
    "                elif connect_bins == (1,1,0,1): # ㅏ\n",
    "                    piece_type.append(4)\n",
    "                elif connect_bins == (1,1,1,0): # ㅓ\n",
    "                    piece_type.append(5)\n",
    "                elif connect_bins == (1,0,0,1): # ㄴ\n",
    "                    piece_type.append(6)\n",
    "                elif connect_bins == (1,0,1,1): # ㅗ\n",
    "                    piece_type.append(7)\n",
    "                elif connect_bins == (1,0,1,0): # ┘\n",
    "                    piece_type.append(8)\n",
    "                elif connect_bins == (1,1,1,1): # +\n",
    "                    piece_type.append(9)\n",
    "                else: # unknown\n",
    "                    piece_type.append(0)\n",
    "            piece_types.append(piece_type)\n",
    "        piece_types = torch.LongTensor(piece_types).to(device)\n",
    "        return piece_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c49d4d-5bcb-4782-b355-b89d13b92f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsawformer = LitJigsawFormer(model, model_config)\n",
    "# lit_jigsawformer = LitJigsawFormer.load_from_checkpoint('./checkpoints/jigsawformer-medium-256-stage2-fold_idx=0-epoch=90-val_score_end=0.9879.ckpt',model=model, config=model_config, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0b061-abe9-4913-bbc1-50dc41501148",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_score_end',\n",
    "    mode='max',\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='jigsawformer-medium-256-stage2-fold_idx=0-{epoch:02d}-{val_score_end:.4f}',\n",
    "    save_top_k=3,\n",
    "    save_weights_only=True\n",
    ")\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_score_end\", mode=\"max\", patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f7dc6-9f48-4e67-91b2-8a3e7483c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=1000, precision='bf16-mixed', callbacks=[checkpoint_callback, earlystopping_callback], val_check_interval=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae9f77-d643-457a-9cde-cd55f9b683d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lit_jigsawformer, train_dataloader, val_dataloader) # tensorboard --logdir=./lightning_logs/version_{} 로 모니터링 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e35c7-cb55-4a7e-b389-ae888cf51a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsawformer = LitJigsawFormer.load_from_checkpoint('./checkpoints/jigsawformer-medium-256-stage2-fold_idx=0-epoch=32-val_score_end=0.9908.ckpt',model=model, config=model_config, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8d8e2-84fa-415d-bb00-b2281c3b4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(precision='bf16-mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f75ec9-4364-4938-9db4-6feaced45bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(lit_jigsawformer, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdac6ed-38a1-4136-9ed7-b5a14034fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = JigsawDataset(\n",
    "    df=test_df,\n",
    "    data_path='./data',\n",
    "    mode='inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4f7d1-c253-4fdc-8133-4b97dfaf599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dfc5f-4003-4b16-b65c-0e24cb294cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataloader = DataLoader(pred_dataset, collate_fn=JigsawCollateFn(inference_transform, 'inference'), batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ca4a0-cd8e-4d0e-8176-51da03ba9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsawformer = LitJigsawFormer.load_from_checkpoint('./checkpoints/jigsawformer-medium-256-stage2-fold_idx=0-epoch=32-val_score_end=0.9908.ckpt',\n",
    "                                                        model=model, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbea6a-9f4a-4fcb-ba70-f3e6086e4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(precision='bf16-mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694654c-a48e-4957-909a-b3fcbdf19e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(lit_jigsawformer, inference_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ae805-7675-420b-b3b7-8935409ecfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8708f-77ee-44e6-9c01-71a413e2999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = np.concatenate(preds) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0331c8-5f8b-4671-bb5f-b6370492d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/jigsawformer-medium-256-stage2-fold_idx=0-val_score_end=0.9908.csv',\n",
    "                  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
